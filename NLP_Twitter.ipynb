{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Mental-Health-Twitter.csv\", encoding='latin-1')\n",
    "df.head()\n",
    "df.shape\n",
    "print (\"Rows     : \" ,df.shape[0])\n",
    "print (\"Columns  : \" ,df.shape[1])\n",
    "print (\"\\nFeatures : \\n\" ,df.columns.tolist())\n",
    "print (\"\\nMissing values :  \", df.isnull().sum().values.sum())\n",
    "print (\"\\nUnique values :  \\n\",df.nunique())\n",
    "df.dtypes\n",
    "print(len(df[df['followers'] < 1000]), 'users with less than 1000 followers')\n",
    "print(len(df[df['followers'] > 1000]), 'userss with more than 1000 followers')\n",
    "df[df['followers'] == df['followers'].max()]['user_id'].iloc[0]\n",
    "top_follower = df[(df['user_id'] == 484109859\n",
    "         )].reset_index(drop=True)\n",
    "top_follower.head()\n",
    "def remove_line_breaks(text):\n",
    "    text = text.replace('\\r', ' ').replace('\\n', ' ')\n",
    "    return text\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    re_replacements = re.compile(\"__[A-Z]+__\") \n",
    "    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n",
    "    '''Escape all the characters in pattern except ASCII letters and numbers'''\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens_zero_punctuation = []\n",
    "    for token in tokens:\n",
    "        if not re_replacements.match(token):\n",
    "            token = re_punctuation.sub(\" \", token)\n",
    "        tokens_zero_punctuation.append(token)\n",
    "    return ' '.join(tokens_zero_punctuation)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def lowercase(text):\n",
    "    text_low = [token.lower() for token in word_tokenize(text)]\n",
    "    return ' '.join(text_low)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    text = \" \".join([word for word in word_tokens if word not in stop])\n",
    "    return text\n",
    "\n",
    "#remove one character words\n",
    "def remove_one_character_words(text):\n",
    "    '''Remove words from dataset that contain only 1 character'''\n",
    "    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      \n",
    "    return ' '.join(text_high_use)   \n",
    "\n",
    "# Stemming is a technique in natural language processing that reduces a word to its base or root form,\n",
    "# which may not be a word by itself but can be used to identify the original word.\n",
    "# Stemming with 'Snowball stemmer\" package\n",
    "def stem(text):\n",
    "    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        \n",
    "    return ' '.join(text_stemmed)\n",
    "\n",
    "def lemma(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    text_lemma = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])       \n",
    "    return ' '.join(text_lemma)\n",
    "\n",
    "#break sentences to individual word list\n",
    "def sentence_word(text):\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    return word_tokens\n",
    "#break paragraphs to sentence token \n",
    "def paragraph_sentence(text):\n",
    "    sent_token = nltk.sent_tokenize(text)\n",
    "    return sent_token    \n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Return a list of words in a text.\"\"\"\n",
    "    return re.findall(r'\\w+', text)\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    no_nums = re.sub(r'\\d+', '', text)\n",
    "    return ''.join(no_nums)\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    _steps = [\n",
    "    remove_line_breaks,\n",
    "    remove_one_character_words,\n",
    "    remove_special_characters,\n",
    "    lowercase,\n",
    "    remove_punctuation,\n",
    "    remove_stopwords,\n",
    "    stem,\n",
    "    remove_numbers\n",
    "]\n",
    "    for step in _steps:\n",
    "        text=step(text)\n",
    "    return text \n",
    "df[\"post_text\"] = df[\"post_text\"].astype(str)\n",
    "df[\"post_text\"] = [x.replace(':',' ') for x in df[\"post_text\"]]\n",
    "\n",
    "df['clean_text'] = pd.Series([clean_text(i) for i in tqdm(df['post_text'])])\n",
    "words = df[\"clean_text\"].values\n",
    "ls = []\n",
    "\n",
    "for i in words:\n",
    "    ls.append(str(i))\n",
    "ls[:5]\n",
    "# The wordcloud \n",
    "plt.figure(figsize=(16,13))\n",
    "wc = WordCloud(background_color=\"lightblue\", colormap='Set2', max_words=1000, max_font_size= 200,  width=1600, height=800)\n",
    "wc.generate(\" \".join(ls))\n",
    "plt.title(\"Most discussed terms\", fontsize=20)\n",
    "plt.imshow(wc.recolor( colormap= 'Set2' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\n",
    "plt.axis('off')\n",
    "all_spam_words = []\n",
    "for sentence in df[df['label'] == 0]['post_text'].to_list():\n",
    "    for word in sentence.split():\n",
    "        all_spam_words.append(word)\n",
    "\n",
    "df = pd.DataFrame(Counter(all_spam_words).most_common(25), columns= ['Word', 'Frequency'])\n",
    "\n",
    "sns.set_context('notebook', font_scale= 1.3)\n",
    "plt.figure(figsize=(18,8))\n",
    "sns.barplot(y = df['Word'], x= df['Frequency'], palette= 'summer')\n",
    "plt.title(\"Most Commonly Used Words\")\n",
    "plt.xlabel(\"Frequnecy\")\n",
    "plt.ylabel(\"Words\")\n",
    "plt.show()\n",
    "df.head()\n",
    "most_pop = df.sort_values('followers', ascending =False)[['user_id', 'followers']].head(12)\n",
    "most_pop['followers1'] = most_pop['followers']/1000\n",
    "plt.figure(figsize = (20,25))\n",
    "\n",
    "sns.barplot(data = most_pop, y = 'user_id', x = 'followers1', color = 'c')\n",
    "plt.xticks(fontsize=27, rotation=0)\n",
    "plt.yticks(fontsize=30, rotation=0)\n",
    "plt.xlabel('User followers in Thousands', fontsize = 21)\n",
    "plt.ylabel('')\n",
    "plt.title('Followers', fontsize = 30);\n",
    "plt.figure(figsize = (20,25))\n",
    "\n",
    "sns.barplot(data = most_pop, y = 'user_id', x = 'followers1', color = 'c')\n",
    "plt.xticks(fontsize=27, rotation=0)\n",
    "plt.yticks(fontsize=30, rotation=0)\n",
    "plt.xlabel('User followers in Thousands', fontsize = 21)\n",
    "plt.ylabel('')\n",
    "plt.title('Followers', fontsize = 30);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
